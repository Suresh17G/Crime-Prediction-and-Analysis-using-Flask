# -*- coding: utf-8 -*-
"""predict-crime.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1i0r_Ue4MCpHOH7vfZNng2KJsMYgORhmr
"""

import numpy as np
import pickle
import pandas as pd
from sklearn.metrics import accuracy_score
from sklearn.preprocessing import MinMaxScaler,LabelEncoder
from keras.models import Sequential
from keras.layers import LSTM, Dense
from sklearn.cluster import KMeans

# Load and preprocess the crime data
crime_data = pd.read_excel("crime_data_2001_2014.xlsx")
crime_data=crime_data.rename(columns={'KIDNAPPING & ABDUCTION':'KIDNAPPING'})
crime_data=crime_data.rename(columns={'DOWRY DEATHS':'DOWRY_DEATHS'})
crime_data=crime_data.rename(columns={'OTHER CRIMES':'OTHER_CRIMES'})
crime_data=crime_data.rename(columns={'TOTAL IPC CRIMES':'TOTAL_CRIMES'})
crime_data

# Split the data into training and testing sets
train_size = int(len(crime_data) * 0.9)
test_size = len(crime_data) - train_size
train_data,test_data=crime_data[:train_size],crime_data[:test_size]

#Split train data into input and output variables
#Divide input into categorical and numerical features for scaling
x_train_s=train_data.iloc[:,0].values
x_train_d=train_data.iloc[:,1].values
x_train_n=train_data.iloc[:,2].values
y_train=train_data.iloc[:,3:].values

#Split test data into input and output variables
#Divide input into categorical and numerical features for scaling
x_test_s=test_data.iloc[:,0].values
x_test_d=test_data.iloc[:,1].values
x_test_n=test_data.iloc[:,2].values
y_test=test_data.iloc[:,3:].values

print(x_train_s.ndim)
print(x_train_d.ndim)
print(x_train_n.ndim)

#Label coding for state categorical features
state_encoder= LabelEncoder()
state_encoder.fit(x_train_s)
x_train_s=state_encoder.transform(x_train_s)
x_test_s=state_encoder.transform(x_test_s)

print(x_train_s.ndim)
print(x_train_d.ndim)
print(x_train_n.ndim)

x_train_d

#Label coding for district categorical features
district_encoder= LabelEncoder()
district_encoder.fit(x_train_d)
x_train_d=district_encoder.transform(x_train_d)
x_test_d=district_encoder.transform(x_test_d)

print(x_train_s.ndim)
print(x_train_d.ndim)
print(x_train_n.ndim)

district_encoder.transform(['CHENNAI'])

# Save the stateEncoder object to a file
with open('state_encoder.pkl', 'wb') as file:
    pickle.dump(state_encoder, file)

# Save the stateEncoder object to a file
with open('district_encoder.pkl', 'wb') as file:
    pickle.dump(district_encoder, file)

# Reshape the 1D array into a 2D array with a single column
x_train_n= np.array(x_train_n).reshape(-1, 1)
x_test_n= np.array(x_test_n).reshape(-1, 1)
x_train_s= np.array(x_train_s).reshape(-1, 1)
x_test_s= np.array(x_test_s).reshape(-1, 1)
x_train_d= np.array(x_train_d).reshape(-1, 1)
x_test_d= np.array(x_test_d).reshape(-1, 1)



#Normalize the numerical feature in input using minmaxscaler
num_scaler = MinMaxScaler(feature_range=(0, 1))
num_scaler.fit(x_train_n)
x_train_n=num_scaler.transform(x_train_n)
x_test_n=num_scaler.transform(x_test_n)

# Save the scaler parameters to a numpy file
np.save('input_scaler.npy', [num_scaler.min_, num_scaler.scale_])

print(x_train_s.ndim)
print(x_train_d.ndim)
print(x_train_n.ndim)

x_train=np.concatenate((x_train_s,x_train_d,x_train_n), axis=1)
x_test=np.concatenate((x_test_s,x_test_d,x_test_n), axis=1)

#Normalize the numerical feature in output using minmaxscaler
result_scaler = MinMaxScaler(feature_range=(0, 1))
result_scaler.fit(y_train)
y_train=result_scaler.transform(y_train)
y_test=result_scaler.transform(y_test)

# Save the scaler parameters to a numpy file
np.save('scaler_params.npy', [result_scaler.min_, result_scaler.scale_])

# Reshape the input data to fit the LSTM model
x_train = np.reshape(x_train, (x_train.shape[0],1,x_train.shape[1]))
x_test = np.reshape(x_test, (x_test.shape[0],1,x_test.shape[1]))



# Build the LSTM model
model = Sequential()
model.add(LSTM(50, input_shape=(x_train.shape[1],x_train.shape[2])))
model.add(Dense(7))
model.compile(loss='mean_squared_error', optimizer='adam')

y_train

# Train the model
model.fit(x_train, y_train, epochs=50, batch_size=32, verbose=1)

# Make predictions
train_predict = model.predict(x_train)
test_predict = model.predict(x_test)

# Reshape the predicted outputs
train_predict = train_predict.reshape(-1, 7)
test_predict = test_predict.reshape(-1, 7)

y_test

# Inverse transform the predictions
train_predict =result_scaler.inverse_transform(train_predict)
y_train =result_scaler.inverse_transform(y_train)
test_predict = result_scaler.inverse_transform(test_predict)
y_test = result_scaler.inverse_transform(y_test)

print(y_test)

print(int(test_predict[0][0]))

# Evaluate the model
train_score = np.sqrt(np.mean((train_predict - y_train) ** 2))
test_score = np.sqrt(np.mean((test_predict - y_test) ** 2))
print(f'Train Score: {train_score:.2f} RMSE')
print(f'Test Score: {test_score:.2f} RMSE')

model.save('crime_predict.h5')







# Perform clustering on crime hotspots
kmeans = KMeans(n_clusters=5)  # Adjust the number of clusters as needed
kmeans.fit(predictions)

# Retrieve cluster labels for each prediction
cluster_labels = kmeans.labels_

# Print the cluster labels and corresponding crime counts
for label, count in zip(cluster_labels, predictions):
    print(f"Cluster: {label}, Crime Count: {count}")